# FineTuning_Llama_using_QLoRA

This repository contains a Google Colab notebook that demonstrates the fine-tuning of a Llama model using QLoRA (Quantized Low-Rank Adapter). QLoRA is an efficient method for adapting pre-trained language models to specific tasks by using low-rank adapters and quantization techniques, which significantly reduce the computational requirements without compromising performance.
